# ========================================================================
# HR Attrition Predictor - Model Configuration
# ========================================================================
# Comprehensive hyperparameter configuration for all ML models
# Optimized for HR attrition prediction with employee datasets
# Author: Mohd Faraz
# Date: September 2025
# ========================================================================

# ===================
# GENERAL ML SETTINGS
# ===================
general:
  # Data splitting
  test_size: 0.2
  validation_size: 0.15
  random_state: 42
  stratify: true
  
  # Cross-validation
  cv_folds: 5
  cv_scoring: 'roc_auc'
  cv_shuffle: true
  
  # Feature selection
  feature_selection: true
  max_features: 50
  feature_importance_threshold: 0.01

# ========================
# LOGISTIC REGRESSION
# ========================
logistic_regression:
  # Core parameters
  C: 1.0                    # Regularization strength (smaller = more regularization)
  penalty: 'l2'             # Regularization type: 'l1', 'l2', 'elasticnet', 'none'
  solver: 'liblinear'       # Algorithm: 'liblinear', 'lbfgs', 'newton-cg', 'sag', 'saga'
  max_iter: 1000           # Maximum iterations for convergence
  
  # Advanced settings
  fit_intercept: true       # Include intercept term
  intercept_scaling: 1      # Scaling for intercept
  class_weight: 'balanced'  # Handle class imbalance automatically
  dual: false              # Dual or primal formulation
  tol: 0.0001              # Tolerance for stopping criteria
  
  # Multi-class settings (if needed)
  multi_class: 'ovr'       # One-vs-rest for binary problems
  
  # Hyperparameter tuning ranges
  hyperparameter_grid:
    C: [0.01, 0.1, 1.0, 10.0, 100.0]
    penalty: ['l1', 'l2']
    solver: ['liblinear', 'saga']
    max_iter: [500, 1000, 2000]

# ==================
# XGBOOST CLASSIFIER
# ==================
xgboost:
  # Tree parameters
  n_estimators: 100         # Number of boosting rounds
  max_depth: 6             # Maximum tree depth (3-10 typical for tabular data)
  min_child_weight: 1      # Minimum sum of instance weights in child (1-10)
  gamma: 0                 # Minimum loss reduction for split (0-5)
  
  # Learning parameters  
  learning_rate: 0.1       # Step size shrinkage (0.01-0.3)
  subsample: 0.8          # Subsample ratio of training instances (0.5-1.0)
  colsample_bytree: 0.8   # Subsample ratio of features (0.5-1.0)
  colsample_bylevel: 1.0  # Subsample ratio per level (0.5-1.0)
  colsample_bynode: 1.0   # Subsample ratio per node (0.5-1.0)
  
  # Regularization
  reg_alpha: 0            # L1 regularization (0-10)
  reg_lambda: 1           # L2 regularization (0-10)
  
  # Performance settings
  n_jobs: -1              # Use all available cores
  random_state: 42        # Reproducibility
  verbosity: 0            # Silent training
  
  # Early stopping
  early_stopping_rounds: 10
  eval_metric: 'logloss'
  
  # Advanced parameters
  max_delta_step: 0       # Maximum delta step (0-10 for imbalanced data)
  scale_pos_weight: 1     # Balance positive/negative weights
  
  # Tree method
  tree_method: 'hist'     # Tree construction algorithm
  grow_policy: 'depthwise' # How to grow trees
  
  # Hyperparameter tuning ranges
  hyperparameter_grid:
    n_estimators: [50, 100, 200, 300]
    max_depth: [3, 4, 5, 6, 7, 8]
    learning_rate: [0.01, 0.05, 0.1, 0.2, 0.3]
    subsample: [0.6, 0.7, 0.8, 0.9, 1.0]
    colsample_bytree: [0.6, 0.7, 0.8, 0.9, 1.0]
    min_child_weight: [1, 3, 5, 7]
    gamma: [0, 0.1, 0.2, 0.5, 1.0]
    reg_alpha: [0, 0.01, 0.1, 1.0]
    reg_lambda: [0, 0.01, 0.1, 1.0]

# =================
# RANDOM FOREST
# =================
random_forest:
  # Tree parameters
  n_estimators: 100        # Number of trees (50-500)
  max_depth: 10           # Maximum tree depth (5-20)
  min_samples_split: 2    # Minimum samples to split node (2-20)
  min_samples_leaf: 1     # Minimum samples in leaf (1-10)
  max_features: 'sqrt'    # Features to consider: 'sqrt', 'log2', float, int
  
  # Bootstrap settings
  bootstrap: true         # Use bootstrap sampling
  oob_score: true        # Use out-of-bag score
  
  # Performance settings
  n_jobs: -1             # Use all cores
  random_state: 42       # Reproducibility
  
  # Advanced parameters
  max_samples: null      # Bootstrap sample size
  min_weight_fraction_leaf: 0.0  # Minimum weighted fraction
  max_leaf_nodes: null   # Maximum leaf nodes
  min_impurity_decrease: 0.0     # Minimum impurity decrease
  
  # Class balancing
  class_weight: 'balanced'  # Handle imbalanced classes
  
  # Hyperparameter tuning ranges
  hyperparameter_grid:
    n_estimators: [50, 100, 200, 300]
    max_depth: [5, 10, 15, 20, null]
    min_samples_split: [2, 5, 10, 15]
    min_samples_leaf: [1, 2, 4, 8]
    max_features: ['sqrt', 'log2', 0.5, 0.7]
    bootstrap: [true, false]

# ==================
# SUPPORT VECTOR MACHINE
# ==================
svm:
  # Core parameters
  C: 1.0                  # Regularization parameter
  kernel: 'rbf'           # Kernel type: 'linear', 'poly', 'rbf', 'sigmoid'
  gamma: 'scale'          # Kernel coefficient: 'scale', 'auto', float
  degree: 3               # Polynomial kernel degree
  
  # Advanced settings
  coef0: 0.0             # Independent term in kernel
  shrinking: true        # Use shrinking heuristic
  probability: true      # Enable probability estimates
  tol: 0.001            # Tolerance for stopping
  cache_size: 200       # Cache size in MB
  class_weight: 'balanced'  # Handle class imbalance
  max_iter: -1          # No limit on iterations
  
  # Hyperparameter tuning ranges
  hyperparameter_grid:
    C: [0.1, 1.0, 10.0, 100.0]
    kernel: ['linear', 'rbf', 'poly']
    gamma: ['scale', 'auto', 0.001, 0.01, 0.1, 1.0]
    degree: [2, 3, 4, 5]

# ===================
# GRADIENT BOOSTING
# ===================
gradient_boosting:
  # Boosting parameters
  n_estimators: 100       # Number of boosting stages
  learning_rate: 0.1      # Learning rate (0.01-0.3)
  max_depth: 3           # Maximum tree depth (3-8)
  min_samples_split: 2   # Minimum samples to split
  min_samples_leaf: 1    # Minimum samples in leaf
  
  # Subsampling
  subsample: 1.0         # Fraction of samples for fitting
  max_features: null     # Features to consider for splits
  
  # Advanced parameters
  loss: 'log_loss'       # Loss function for classification
  criterion: 'friedman_mse'  # Split quality measure
  min_weight_fraction_leaf: 0.0
  max_leaf_nodes: null
  min_impurity_decrease: 0.0
  
  # Performance settings
  random_state: 42
  validation_fraction: 0.1
  n_iter_no_change: null
  tol: 0.0001
  
  # Hyperparameter tuning ranges
  hyperparameter_grid:
    n_estimators: [50, 100, 200]
    learning_rate: [0.01, 0.05, 0.1, 0.2]
    max_depth: [3, 4, 5, 6]
    min_samples_split: [2, 5, 10]
    min_samples_leaf: [1, 2, 4]
    subsample: [0.8, 0.9, 1.0]

# ==================
# NEURAL NETWORK (MLP)
# ==================
neural_network:
  # Architecture
  hidden_layer_sizes: [100, 50]  # Hidden layer sizes
  activation: 'relu'      # Activation: 'identity', 'logistic', 'tanh', 'relu'
  solver: 'adam'         # Solver: 'lbfgs', 'sgd', 'adam'
  
  # Regularization
  alpha: 0.0001          # L2 penalty (0.0001-0.01)
  
  # Training parameters
  learning_rate: 'constant'  # Learning rate schedule
  learning_rate_init: 0.001  # Initial learning rate
  max_iter: 200          # Maximum iterations
  batch_size: 'auto'    # Batch size for SGD
  
  # Advanced settings
  shuffle: true          # Shuffle samples
  random_state: 42      # Reproducibility
  tol: 0.0001           # Tolerance for optimization
  warm_start: false     # Reuse previous solution
  momentum: 0.9         # Momentum for SGD
  nesterovs_momentum: true  # Use Nesterov's momentum
  early_stopping: false     # Early stopping
  validation_fraction: 0.1  # Validation set fraction
  beta_1: 0.9           # Adam parameter
  beta_2: 0.999         # Adam parameter
  epsilon: 1e-08        # Adam parameter
  
  # Hyperparameter tuning ranges
  hyperparameter_grid:
    hidden_layer_sizes: [[50], [100], [50, 30], [100, 50], [100, 50, 25]]
    activation: ['relu', 'tanh', 'logistic']
    alpha: [0.0001, 0.001, 0.01, 0.1]
    learning_rate_init: [0.001, 0.01, 0.1]
    solver: ['adam', 'sgd']

# ===================
# ENSEMBLE METHODS
# ===================
ensemble:
  # Voting Classifier
  voting:
    voting: 'soft'        # 'hard' or 'soft' voting
    n_jobs: -1           # Parallel processing
    
  # Bagging
  bagging:
    n_estimators: 10     # Number of base estimators
    max_samples: 1.0     # Samples to draw for training
    max_features: 1.0    # Features to draw for training
    bootstrap: true      # Bootstrap sampling
    bootstrap_features: false  # Bootstrap features
    oob_score: false     # Use out-of-bag score
    warm_start: false    # Reuse previous solution
    n_jobs: -1          # Parallel processing
    random_state: 42    # Reproducibility
    
  # AdaBoost
  adaboost:
    n_estimators: 50     # Number of estimators
    learning_rate: 1.0   # Learning rate
    algorithm: 'SAMME.R'  # Algorithm: 'SAMME', 'SAMME.R'
    random_state: 42     # Reproducibility

# =====================
# HYPERPARAMETER TUNING
# =====================
hyperparameter_tuning:
  # Search strategy
  search_method: 'random'  # 'grid', 'random', 'bayesian'
  
  # Grid/Random search settings
  n_iter: 50              # Random search iterations
  cv: 5                   # Cross-validation folds
  scoring: 'roc_auc'      # Scoring metric
  n_jobs: -1              # Parallel processing
  random_state: 42        # Reproducibility
  
  # Advanced search settings
  refit: true             # Refit best estimator
  return_train_score: true # Return training scores
  
  # Bayesian optimization (if using Optuna)
  bayesian:
    n_trials: 100         # Number of optimization trials
    timeout: 3600         # Timeout in seconds (1 hour)
    direction: 'maximize'  # Optimization direction
    sampler: 'TPE'        # Tree-structured Parzen Estimator

# ================
# MODEL EVALUATION
# ================
evaluation:
  # Metrics to calculate
  metrics:
    - 'accuracy'
    - 'precision'
    - 'recall'
    - 'f1_score'
    - 'roc_auc'
    - 'log_loss'
    - 'confusion_matrix'
    
  # Cross-validation settings
  cv_metrics: true        # Calculate CV metrics
  cv_std: true           # Include standard deviations
  
  # Threshold optimization
  threshold_optimization: true
  threshold_metric: 'f1_score'
  
  # Feature importance
  calculate_feature_importance: true
  top_features_count: 20

# ==================
# MODEL PERSISTENCE
# ==================
model_saving:
  # Save settings
  save_best_model: true
  save_all_models: false
  model_format: 'joblib'    # 'joblib', 'pickle'
  
  # Versioning
  version_models: true
  model_registry: false     # Use MLflow model registry
  
  # Compression
  compress_models: true
  compression_level: 3      # 0-9 compression level

# =================
# FEATURE ENGINEERING
# =================
feature_engineering:
  # Scaling
  scaling_method: 'standard'  # 'standard', 'minmax', 'robust', 'quantile'
  
  # Encoding
  categorical_encoding: 'onehot'  # 'onehot', 'target', 'ordinal', 'binary'
  handle_unknown: 'ignore'        # How to handle unknown categories
  
  # Feature creation
  create_interactions: true       # Create feature interactions
  polynomial_features: false     # Create polynomial features
  polynomial_degree: 2          # Degree for polynomial features
  
  # Feature selection
  feature_selection_method: 'rfe'  # 'rfe', 'univariate', 'mutual_info'
  selection_k_best: 20            # Number of best features to select

# ==================
# DEPLOYMENT SETTINGS
# ==================
deployment:
  # Model serving
  batch_prediction: true
  real_time_prediction: true
  
  # Performance requirements
  max_prediction_time_ms: 100  # Maximum prediction time
  memory_limit_mb: 512         # Memory limit for model
  
  # Monitoring
  performance_monitoring: true
  drift_detection: true
  alert_thresholds:
    accuracy_drop: 0.05        # Alert if accuracy drops by 5%
    prediction_time_ms: 200    # Alert if prediction takes >200ms
    memory_usage_mb: 600       # Alert if memory usage exceeds 600MB